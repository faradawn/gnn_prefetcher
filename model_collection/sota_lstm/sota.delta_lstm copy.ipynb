{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check new_1001 equals 1001 classes True\n",
      "Delta classes unique 1001\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "# ingore gpu\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Flatten, Embedding, LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.layers import Dense, Activation, TimeDistributed, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "def my_dict(train_trace, top_num=1000):\n",
    "    # train_trace is a pandas df\n",
    "    train_trace['ByteOffset_Delta'] = train_trace['ByteOffset'] - \\\n",
    "        train_trace['ByteOffset'].shift(-1)\n",
    "    train_trace['ByteOffset_Delta'] = train_trace['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "    x = Counter(train_trace['ByteOffset_Delta'])\n",
    "    vals = x.most_common(top_num)\n",
    "    top_deltas = {}\n",
    "    rev_map = {}\n",
    "    for i, tup in enumerate(vals):\n",
    "        top_deltas[tup[0]] = i\n",
    "        rev_map[i] = tup[0] # i -> raw delta\n",
    "    \n",
    "    forward_map = {}\n",
    "    count = 0\n",
    "    while (count < len(train_trace)):\n",
    "        x = train_trace['ByteOffset_Delta'].iloc[count]\n",
    "        if x in top_deltas:\n",
    "            forward_map[x] = top_deltas[x]\n",
    "        count += 1\n",
    "    return forward_map, rev_map\n",
    "\n",
    "input_file = '/home/cc/flashnet/model_collection/5_block_prefetching/dataset/iotrace/alibaba.cut.per_10k.rw_80_20.723/read_io.trace'\n",
    "# ts_record,dev_num,offset,size,io_type\n",
    "df = pd.read_csv(input_file, sep=',')\n",
    "\n",
    "# Their method\n",
    "\n",
    "df['IOSize_log'] = np.log2(df['size'])\n",
    "df['IOSize_log_roundoff']= round(df['IOSize_log'])\n",
    "\n",
    "\n",
    "# print(len(Counter(df['IOSize'])))\n",
    "# print(len(Counter(df['IOSize_log'])))\n",
    "# print(len(Counter(df['IOSize_log_roundoff'])))\n",
    "\n",
    "# Cell\n",
    "\n",
    "df['ByteOffset_Delta'] = df['offset'] - df['offset'].shift(-1)\n",
    "df = df.drop(df.index[-1])\n",
    "\n",
    "# Cell\n",
    "\n",
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "a = df['ByteOffset_Delta'].unique().tolist()\n",
    "\n",
    "# Reverse map 1\n",
    "rev_map_1 = {}\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): \n",
    "    operation_id_map[id] = i \n",
    "    rev_map_1[i] = id\n",
    "df['ByteOffset_Delta_class'] = df['ByteOffset_Delta'].map(lambda x: operation_id_map[x])\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "x = Counter(df['ByteOffset_Delta_class'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000) # list of tuples \n",
    "bo_list = [] # Top 1000 deltas \n",
    "\n",
    "for x in vals:\n",
    "    bo_list.append(x[0])\n",
    "        \n",
    "count = 0\n",
    "label_list = []\n",
    "\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta_class'].iloc[count]\n",
    "    raw_delta = df['ByteOffset_Delta'].iloc[count]\n",
    "    if x in bo_list:\n",
    "        label_list.append(x)\n",
    "    else:\n",
    "        operation_id_map[raw_delta] = 999999\n",
    "        label_list.append(999999)\n",
    "    count= count + 1\n",
    "    \n",
    "ByteOffset_Delta_class_backup  = df['ByteOffset_Delta_class'] \n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "\n",
    "# Reverse map 2\n",
    "a = df['ByteOffset_Delta_class'].unique().tolist()\n",
    "bo_map = {}\n",
    "rev_map_2 = {}\n",
    "for i,id in enumerate(a): \n",
    "    rev_map_2[i] = id\n",
    "    bo_map[id] = i \n",
    "\n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta_class'].map(lambda x: bo_map[x])\n",
    "\n",
    "# My addition forward_map that combines the two small maps\n",
    "forward_map = {}\n",
    "count = 0\n",
    "while(count < len(df)):\n",
    "    raw_delta = df['ByteOffset_Delta'].iloc[count]\n",
    "    forward_map[raw_delta] = bo_map[operation_id_map[raw_delta]]\n",
    "    count += 1\n",
    "\n",
    "# Check i\n",
    "df['new_1001'] = df['ByteOffset_Delta'].map(lambda x: forward_map[x])\n",
    "print(\"Check new_1001 equals 1001 classes\", df['new_1001'].equals(df['ByteOffset_Delta_Class_1001']))\n",
    "\n",
    "\n",
    "# CHeck answer (PASSED)\n",
    "# rev_col = []\n",
    "# for value in df['ByteOffset_Delta_Class_1001']:\n",
    "#     if rev_map_2[value] == 999999:\n",
    "#         rev_col.append(np.nan)\n",
    "#     else:\n",
    "#         rev_col.append(rev_map_1[rev_map_2[value]])\n",
    "# df['rev_col'] = rev_col\n",
    "\n",
    "# label_list = df['ByteOffset_Delta_Class_1001']     \n",
    "# df['ByteOffset_Delta_Class_1001']  = label_list\n",
    "# df = df.drop(df.index[0]) # Problem\n",
    "\n",
    "a = df['IOSize_log_roundoff'].unique().tolist()\n",
    "size_map = {}\n",
    "rev_map_3 = {}\n",
    "for i,id in enumerate(a):\n",
    "    rev_map_3[i] = id\n",
    "    size_map[id] = i \n",
    "df['Size_Class'] = df['IOSize_log_roundoff'].map(lambda x: size_map[x])\n",
    "\n",
    "\n",
    "print(\"Delta classes unique\", df['ByteOffset_Delta_Class_1001'].nunique())\n",
    "\n",
    "# Save the three maps\n",
    "import pickle\n",
    "# with open('./fara_saved_models/rev_map_1.pkl', 'wb') as file:\n",
    "#     pickle.dump(rev_map_1, file)\n",
    "# with open('./fara_saved_models/rev_map_2.pkl', 'wb') as file:\n",
    "#     pickle.dump(rev_map_2, file)\n",
    "# with open('./fara_saved_models/rev_map_3.pkl', 'wb') as file:\n",
    "#     pickle.dump(rev_map_3, file)\n",
    "\n",
    "# print(\"reverse maps saved\")\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'offset',\n",
    "    'size',\n",
    "    'IOSize_log_roundoff',\n",
    "    'ByteOffset_Delta_Class_1001',\n",
    "    'Size_Class'\n",
    "]\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lba (6007, 32), X_test_lba (1959, 32), X_train_size (6007, 32), X_test_size (1959, 32)\n",
      "y_train_lba (6007, 32, 1), y_test_lba (1959, 32, 1), y_train_size (6007, 32, 1), y_test_size (1959, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split to train, validate and test\n",
    "import math\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_Class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_Class'].tolist()\n",
    "\n",
    "lba_train= np.array(lba_train).reshape(-1,1) # each row only one element\n",
    "lba_test= np.array(lba_test).reshape(-1,1)\n",
    "size_train= np.array(size_train).reshape(-1,1)\n",
    "size_test= np.array(size_test).reshape(-1,1)\n",
    "\n",
    "# Each x is [1, ..., 32] and y is [33, ..., 65] (using 32 words to predict the next 32 words)\n",
    "def create_dataset2(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - 2 * window_size):\n",
    "        a = dataset[i:(i + window_size), 0] # a is [1,2,3,4 ...], next [2,3,4,5 ...]\n",
    "        dataX.append(a)\n",
    "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
    "        dataY.append(b) # \n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "lstm_num_timesteps = 32\n",
    "    \n",
    "X_train_lba, y_train_lba = create_dataset2(lba_train, lstm_num_timesteps)\n",
    "X_test_lba, y_test_lba = create_dataset2(lba_test, lstm_num_timesteps)\n",
    "X_train_size, y_train_size = create_dataset2(size_train, lstm_num_timesteps)\n",
    "X_test_size, y_test_size = create_dataset2(size_test, lstm_num_timesteps)\n",
    "\n",
    "lstm_num_features = 1\n",
    "lstm_predict_sequences = True\n",
    "lstm_num_predictions = 32\n",
    "\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    \n",
    "y_train_lba = np.reshape(y_train_lba, (y_train_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba = np.reshape(y_test_lba, (y_test_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_train_size = np.reshape(y_train_size, (y_train_size.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_size = np.reshape(y_test_size, (y_test_size.shape[0], lstm_num_predictions, lstm_num_features))                        \n",
    "\n",
    "print(f'X_train_lba {X_train_lba.shape}, X_test_lba {X_test_lba.shape}, X_train_size {X_train_size.shape}, X_test_size {X_test_size.shape}')\n",
    "print(f'y_train_lba {y_train_lba.shape}, y_test_lba {y_test_lba.shape}, y_train_size {y_train_size.shape}, y_test_size {y_test_size.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 1 Delta_Class 1001\n",
      "vocab 2 Size_class 7\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 32)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 32)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)     (None, 32, 200)              200200    ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)     (None, 32, 200)              1400      ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 32, 400)              0         ['embedding_6[0][0]',         \n",
      " )                                                                   'embedding_7[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               (None, 32, 200)              480800    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               (None, 32, 200)              320800    ['lstm_4[0][0]']              \n",
      "                                                                                                  \n",
      " offset (TimeDistributed)    (None, 32, 1001)             201201    ['lstm_5[0][0]']              \n",
      "                                                                                                  \n",
      " iosize (TimeDistributed)    (None, 32, 7)                1407      ['lstm_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1205808 (4.60 MB)\n",
      "Trainable params: 1205808 (4.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "\n",
    "maxlen= 32\n",
    "\n",
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(32,))\n",
    "# # inputA = Sequential()\n",
    "# # inputB = Sequential()\n",
    "vocabulary_1 = df['ByteOffset_Delta_Class_1001'].nunique()\n",
    "vocabulary_2 = df['Size_Class'].nunique()\n",
    "\n",
    "print(\"vocab 1 Delta_Class\", vocabulary_1)\n",
    "print(\"vocab 2 Size_class\", vocabulary_2)\n",
    "\n",
    "hidden_size = 200 # original 1500\n",
    "\n",
    "# input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "inputA=Input(shape=(maxlen,),dtype='float64')  \n",
    "inputB=Input(shape=(maxlen,),dtype='float64') \n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "lstm1 = LSTM(hidden_size,return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "# create classification output\n",
    "offset = TimeDistributed(Dense(units=vocabulary_1, activation='softmax'), name='offset')(lstm2)\n",
    "iosize = TimeDistributed(Dense(units=vocabulary_2, activation='softmax'), name='iosize')(lstm2)\n",
    "\n",
    "model =Model([inputA,inputB],[offset,iosize]) # combining all into a Keras model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'offset': 'sparse_categorical_crossentropy', 'iosize': 'sparse_categorical_crossentropy'},\n",
    "              loss_weights={'offset': 2., 'iosize': 1.5},\n",
    "              metrics={ 'offset': 'categorical_accuracy', 'iosize': 'categorical_accuracy'})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 28s 133ms/step - loss: 10.2832 - offset_loss: 4.0895 - iosize_loss: 1.4028 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6981 - val_loss: 8.5417 - val_offset_loss: 3.1606 - val_iosize_loss: 1.4804 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.0901\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 23s 124ms/step - loss: 9.5293 - offset_loss: 3.7538 - iosize_loss: 1.3477 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.7270 - val_loss: 8.3525 - val_offset_loss: 3.1176 - val_iosize_loss: 1.4115 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.8104\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 23s 123ms/step - loss: 9.3068 - offset_loss: 3.6641 - iosize_loss: 1.3191 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.7015 - val_loss: 8.4324 - val_offset_loss: 3.1512 - val_iosize_loss: 1.4200 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.9574\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 9.0619 - offset_loss: 3.5638 - iosize_loss: 1.2896 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6652 - val_loss: 8.4485 - val_offset_loss: 3.1573 - val_iosize_loss: 1.4226 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.9888\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 8.7649 - offset_loss: 3.4405 - iosize_loss: 1.2559 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6465 - val_loss: 8.4828 - val_offset_loss: 3.1794 - val_iosize_loss: 1.4161 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.9242\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 8.4265 - offset_loss: 3.2961 - iosize_loss: 1.2228 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6384 - val_loss: 8.4966 - val_offset_loss: 3.1836 - val_iosize_loss: 1.4196 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.9170\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 8.0665 - offset_loss: 3.1426 - iosize_loss: 1.1875 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6404 - val_loss: 8.5713 - val_offset_loss: 3.2044 - val_iosize_loss: 1.4417 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.9652\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 23s 121ms/step - loss: 7.6913 - offset_loss: 2.9791 - iosize_loss: 1.1554 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6353 - val_loss: 8.6902 - val_offset_loss: 3.2453 - val_iosize_loss: 1.4664 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.8707\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 7.2909 - offset_loss: 2.8045 - iosize_loss: 1.1213 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6248 - val_loss: 8.7920 - val_offset_loss: 3.2955 - val_iosize_loss: 1.4674 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.8145\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 6.8661 - offset_loss: 2.6195 - iosize_loss: 1.0848 - offset_categorical_accuracy: 0.0000e+00 - iosize_categorical_accuracy: 0.6153 - val_loss: 8.8670 - val_offset_loss: 3.3096 - val_iosize_loss: 1.4986 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.6198\n",
      "Done, elapsed 234.51370787620544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import time\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "data_path = r'/home/aakashkapoor103/anaconda3/Data/Models/'\n",
    "monitor = EarlyStopping(monitor='loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "# checkpointer = ModelCheckpoint(filepath=data_path + '/src1_1_model-trained.hdf5', verbose=1)\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "valid = [X_test_lba,X_test_size],[y_test_lba,y_test_size]\n",
    "\n",
    "model.fit([X_train_lba,X_train_size],[y_train_lba,y_train_size],\n",
    "          verbose=1, epochs=num_epochs, callbacks=[monitor], batch_size=batch_size, validation_data=valid)\n",
    "\n",
    "# model.load_weights('best_weights_src1.hdf5') # load weights from best model\n",
    "model.save(f'./sota_lstm/lstm_hidden_{hidden_size}_epochs_{num_epochs}.h5')\n",
    "print('Done, elapsed', time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 3s 42ms/step\n",
      "pred1 pred2 shape (1959, 32, 1001) (1959, 32, 7)\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "hidden_size = 200\n",
    "num_epochs = 10\n",
    "new_model = keras.models.load_model(f'./sota_lstm/lstm_hidden_{hidden_size}_epochs_{num_epochs}.h5')\n",
    "pred1,pred2 = new_model.predict([X_test_lba,X_test_size],verbose =1 )\n",
    "\n",
    "print(\"pred1 pred2 shape\", pred1.shape, pred2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans pred_1 shape 1959 test_final shape 1959\n",
      "IO Size Accuracy 0.45227156712608474\n",
      "Best LBA Delta Accuracy 0.38693210821847884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "\n",
    "# Create data\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_Class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_Class'].tolist()\n",
    "\n",
    "# ===\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lba_test_final = lba_test[-(len(pred1)):]\n",
    "lba_size_final = size_test[-(len(pred1)):]\n",
    "\n",
    "max_lba_accuracy = 0\n",
    "max_lba_accuracy_pos = 0\n",
    "\n",
    "max_size_accuracy = 100000\n",
    "max_size_accuracy_pos = 0\n",
    "\n",
    "pred_1 = pred1[:,0,:] # select the first prediction (1959, 1)\n",
    "pred_2 = pred2[:,0,:] # select the first prediction (1959, 1)\n",
    "pred_1 = np.argmax(pred_1, axis=1)\n",
    "pred_2 = np.argmax(pred_2, axis=1)\n",
    "\n",
    "print(\"Trans pred_1 shape\", len(pred_1), \"test_final shape\", len(lba_test_final))\n",
    "# for i in range(len(pred_1)):\n",
    "#     print(f\"i {i}, pred_addr {pred_1[i]}, pred_size {pred_2[i]}\")\n",
    "                 \n",
    "\n",
    "lba_accuracy = accuracy_score(lba_test_final, pred_1)\n",
    "size_accuracy = accuracy_score(lba_size_final, pred_2)\n",
    "\n",
    "print(\"IO Size Accuracy\", str(size_accuracy))\n",
    "print(\"Best LBA Delta Accuracy\", str(lba_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record\n",
    "'''\n",
    "=== Test 1\n",
    "hidden_size = 150\n",
    "num_epochs = 10\n",
    "Done, elapsed 150\n",
    "\n",
    "IO Size Accuracy 0.39\n",
    "Best LBA Delta Accuracy 0.35\n",
    "\n",
    "=== Test 2, same acc but slower\n",
    "hidden_size = 150\n",
    "num_epochs = 30\n",
    "Done, elapsed 1019.8938829898834\n",
    "\n",
    "IO Size Accuracy 0.4058192955589586\n",
    "Best LBA Delta Accuracy 0.35324144971924454\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw deltas 3785 -> Classes 1000 -> Coverage 0.6560415122312825\n",
      "Raw io_size 37  -> Classes 7\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN (My Version)\n",
    "\n",
    "input_file = '/home/daniar/flashnet/model_collection/5_block_prefetching/dataset/iotrace/alibaba.cut.per_10k.rw_80_20.723/read_io.trace'\n",
    "# ts_record,dev_num,offset,size,io_type\n",
    "\n",
    "df = pd.read_csv(input_file, sep=',')\n",
    "\n",
    "# Step 1: Convert address delta into 1001 classes  \n",
    "df['ByteOffset_Delta'] = df['offset'] - df['offset'].shift(-1)\n",
    "df = df.drop(df.index[-1]) # drop the last row\n",
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "x = Counter(df['ByteOffset_Delta'])\n",
    "unique_deltas = len(x)\n",
    "most_common_deltas = x.most_common(1000) # creates a map\n",
    "class_map = {}\n",
    "i = 0\n",
    "coverage = 0\n",
    "for delta, cnt in most_common_deltas:\n",
    "    class_map[delta] = i\n",
    "    i += 1\n",
    "    coverage += cnt\n",
    "\n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta'].map(lambda x : class_map.get(x, 1001)) # If in map, assign the class. Else, assign class 1001\n",
    "\n",
    "# Step 2: Convert io_size into classes\n",
    "df['IOSize_log'] = np.log2(df['size'])\n",
    "df['IOSize_log_roundoff']= round(df['IOSize_log'])\n",
    "a = df['IOSize_log_roundoff'].unique().tolist()\n",
    "size_map = {}\n",
    "for i,id in enumerate(a): size_map[id] = i \n",
    "df['Size_Class'] = df['IOSize_log_roundoff'].map(lambda x: size_map[x])\n",
    "\n",
    "# print(\"Before drop: {}\".format(df.columns))\n",
    "columns_to_keep = [\n",
    "    'offset',\n",
    "    'size',\n",
    "    'IOSize_log_roundoff',\n",
    "    'ByteOffset_Delta_Class_1001',\n",
    "    'Size_Class'\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "df = df.drop(df.index[0]) # Is it necessary?\n",
    "\n",
    "print(\"Raw deltas\", len(x), \"-> Classes\", df['ByteOffset_Delta_Class_1001'].nunique(), \"-> Coverage\", coverage / len(df))\n",
    "print(\"Raw io_size\", len(Counter(df['size'])), \" -> Classes\", len(Counter(df['IOSize_log_roundoff'])))\n",
    "\n",
    "# print(\"Printing the df\")\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>size</th>\n",
       "      <th>IOSize_log_roundoff</th>\n",
       "      <th>ByteOffset_Delta_Class_1001</th>\n",
       "      <th>Size_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6657662976</td>\n",
       "      <td>16384</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6657683456</td>\n",
       "      <td>4096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6657679360</td>\n",
       "      <td>4096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>987103232</td>\n",
       "      <td>16384</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>987222016</td>\n",
       "      <td>8192</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>987217920</td>\n",
       "      <td>4096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6646398976</td>\n",
       "      <td>16384</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6646427648</td>\n",
       "      <td>8192</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6646423552</td>\n",
       "      <td>4096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4440633344</td>\n",
       "      <td>4096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        offset   size  IOSize_log_roundoff  ByteOffset_Delta_Class_1001  \\\n",
       "1   6657662976  16384                 14.0                            5   \n",
       "2   6657683456   4096                 12.0                            4   \n",
       "3   6657679360   4096                 12.0                          337   \n",
       "4    987103232  16384                 14.0                           43   \n",
       "5    987222016   8192                 13.0                            4   \n",
       "6    987217920   4096                 12.0                          338   \n",
       "7   6646398976  16384                 14.0                            9   \n",
       "8   6646427648   8192                 13.0                            4   \n",
       "9   6646423552   4096                 12.0                          339   \n",
       "10  4440633344   4096                 12.0                          340   \n",
       "\n",
       "    Size_Class  \n",
       "1            1  \n",
       "2            0  \n",
       "3            0  \n",
       "4            1  \n",
       "5            2  \n",
       "6            0  \n",
       "7            1  \n",
       "8            2  \n",
       "9            0  \n",
       "10           0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashnet-explore-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
